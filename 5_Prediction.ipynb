{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fbffe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All labraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "import json\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc1408",
   "metadata": {},
   "source": [
    "### part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23f5e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir good_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51d51ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir bad_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fda43bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path='C://Users//hp//7.Muffassil_Project//3_Wafer_fault_detection//3_Muffassil_code/schema_prediction.json'\n",
    "training_folder_path='C:\\\\Users\\\\hp\\\\7.Muffassil_Project\\\\3_Wafer_fault_detection\\\\3_Muffassil_code\\\\Prediction_Batch_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "896d521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuesfromscheme():\n",
    "    with open (schema_path, 'r') as f:\n",
    "        data=json.load(f)   #conveting str to dict format ie we use json\n",
    "#     print(type(data))\n",
    "#     print(data)\n",
    "    pattern=data['SampleFileName']\n",
    "    LengthOfDateStampInFile=data['LengthOfDateStampInFile']\n",
    "    LengthOfTimeStampInFile=data['LengthOfTimeStampInFile']\n",
    "    NumberofColumns=data['NumberofColumns']\n",
    "    \n",
    "    return pattern,LengthOfDateStampInFile,LengthOfTimeStampInFile,NumberofColumns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8fd7ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern,LengthOfDateStampInFile,LengthOfTimeStampInFile,NumberofColumns=valuesfromscheme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9317d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex():\n",
    "    regex=\"['wafer_']+[\\d_]+[\\d]+\\.csv\"\n",
    "    return regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d49da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex=regex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e853021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationfilename (regex,LengthOfDateStampInFile,LengthOfTimeStampInFile):\n",
    "    files=[i for i in os.listdir(training_folder_path)]\n",
    "    for filename in files:\n",
    "        if re.match(regex,filename):\n",
    "            split_at_dot=re.split('.csv',filename)\n",
    "            split_at_underline=re.split('_',split_at_dot[0])\n",
    "            if len(split_at_underline[1])==LengthOfDateStampInFile:\n",
    "                if len(split_at_underline[2])==LengthOfTimeStampInFile:\n",
    "                    shutil.copy(f'{training_folder_path}\\\\{filename}','good_data_pred')\n",
    "                else:\n",
    "                    shutil.copy(f'{training_folder_path}\\\\{filename}','bad_data_pred')\n",
    "                    \n",
    "            else:\n",
    "                    shutil.copy(f'{training_folder_path}\\\\{filename}','bad_data_pred')\n",
    "                    \n",
    "        else:\n",
    "                    shutil.copy(f'{training_folder_path}\\\\{filename}','good_data_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "579ad302",
   "metadata": {},
   "outputs": [],
   "source": [
    "validationfilename (regex,LengthOfDateStampInFile,LengthOfTimeStampInFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d7f06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating column length in the files\n",
    "\n",
    "def validate_column (NumberofColumns):\n",
    "    files=[i for i in os.listdir('good_data_pred') ]\n",
    "    for filename in files:\n",
    "        csv=pd.read_csv(f'good_data_pred/{filename}')\n",
    "        if csv.shape[1]==NumberofColumns:\n",
    "            pass\n",
    "        else: \n",
    "            shutil.move(f'good_data_pred/{filename}','good_data_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ab95ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_column (NumberofColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c4d48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Missing Value in Whole Column\n",
    "\n",
    "def Validate_missing_value_in_column():\n",
    "    for file in os.listdir('good_data_pred'):\n",
    "        csv=pd.read_csv(f'good_data_pred/{file}')\n",
    "        count=0\n",
    "        for column in csv:\n",
    "            if csv[column].isnull().sum()==len(csv[column]):\n",
    "#           if (len(csv[column])-csv[column].count())== len(csv[column]):\n",
    "                count+=1\n",
    "                shutil.move(f'good_data_pred/{file}','bad_data_pred')\n",
    "                break\n",
    "        if count==0:\n",
    "            csv.rename(columns={'Unnamed: 0':'wafer'}, inplace=True)\n",
    "            csv.to_csv(f'good_data_pred/{file}', index=None, header=True)\n",
    "    \n",
    "    print('[info] validation of column done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3459b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] validation of column done\n"
     ]
    }
   ],
   "source": [
    "Validate_missing_value_in_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b187124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all the file into 1 file\n",
    "list=[]\n",
    "for file in os.listdir('good_data_pred'):\n",
    "    csv=pd.read_csv(f'good_data_pred/{file}')\n",
    "    list.append(csv)\n",
    "    \n",
    "combined=pd.concat(list,axis=0, ignore_index=True)\n",
    "combined.to_csv('pred_main_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4126a86",
   "metadata": {},
   "source": [
    "### part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98325231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sensor-6', 'Sensor-14', 'Sensor-43', 'Sensor-50', 'Sensor-53', 'Sensor-70', 'Sensor-98', 'Sensor-142', 'Sensor-150', 'Sensor-179', 'Sensor-180', 'Sensor-187', 'Sensor-190', 'Sensor-191', 'Sensor-192', 'Sensor-193', 'Sensor-194', 'Sensor-195', 'Sensor-227', 'Sensor-230', 'Sensor-231', 'Sensor-232', 'Sensor-233', 'Sensor-234', 'Sensor-235', 'Sensor-236', 'Sensor-237', 'Sensor-238', 'Sensor-241', 'Sensor-242', 'Sensor-243', 'Sensor-244', 'Sensor-257', 'Sensor-258', 'Sensor-259', 'Sensor-260', 'Sensor-261', 'Sensor-262', 'Sensor-263', 'Sensor-264', 'Sensor-265', 'Sensor-266', 'Sensor-267', 'Sensor-277', 'Sensor-285', 'Sensor-314', 'Sensor-315', 'Sensor-316', 'Sensor-323', 'Sensor-326', 'Sensor-327', 'Sensor-328', 'Sensor-329', 'Sensor-330', 'Sensor-331', 'Sensor-365', 'Sensor-370', 'Sensor-371', 'Sensor-372', 'Sensor-373', 'Sensor-374', 'Sensor-375', 'Sensor-376', 'Sensor-379', 'Sensor-380', 'Sensor-381', 'Sensor-382', 'Sensor-395', 'Sensor-396', 'Sensor-397', 'Sensor-398', 'Sensor-399', 'Sensor-400', 'Sensor-401', 'Sensor-402', 'Sensor-403', 'Sensor-404', 'Sensor-405', 'Sensor-415', 'Sensor-423', 'Sensor-450', 'Sensor-451', 'Sensor-452', 'Sensor-459', 'Sensor-462', 'Sensor-463', 'Sensor-464', 'Sensor-465', 'Sensor-466', 'Sensor-467', 'Sensor-482', 'Sensor-499', 'Sensor-502', 'Sensor-503', 'Sensor-504', 'Sensor-505', 'Sensor-506', 'Sensor-507', 'Sensor-508', 'Sensor-509', 'Sensor-510', 'Sensor-513', 'Sensor-514', 'Sensor-515', 'Sensor-516', 'Sensor-529', 'Sensor-530', 'Sensor-531', 'Sensor-532', 'Sensor-533', 'Sensor-534', 'Sensor-535', 'Sensor-536', 'Sensor-537', 'Sensor-538', 'Sensor-539']\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('pred_main_data.csv')\n",
    "df.head()\n",
    "\n",
    "# remove unnamed column\n",
    "df.drop(columns=['Unnamed: 0','wafer'], inplace=True)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer=KNNImputer(n_neighbors=3,missing_values=np.nan,)\n",
    "new_df=imputer.fit_transform(df)\n",
    "\n",
    "new_df=pd.DataFrame(new_df,columns=df.columns)\n",
    "\n",
    "\n",
    "# checking the column which have a zero standard deviation\n",
    "drop_col=[]\n",
    "describe=new_df.describe()\n",
    "for i in new_df.columns:\n",
    "    if describe[i]['std']==0:\n",
    "        drop_col.append(i)\n",
    "print(drop_col)   \n",
    "\n",
    "new_df.drop(columns=drop_col,inplace=True, axis=1)\n",
    "\n",
    "new_df.to_csv('pred_clean_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ba0ae",
   "metadata": {},
   "source": [
    "### part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "65ae493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "putting prediction points into respective cluster based on the kmeans clustering model taht we used\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import joblib\n",
    "data=pd.read_csv('pred_clean_data.csv')\n",
    "model=joblib.load('models/kmeans.pkl')\n",
    "y_pred=model.predict(data)\n",
    "data['cluster']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f66c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('pred_clusters.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85231e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cluster=data['cluster'].unique()\n",
    "\n",
    "#load all models\n",
    "\n",
    "model0=joblib.load('models/Random_forest_0.pkl')\n",
    "model1=joblib.load('models/xgboost_1.pkl')\n",
    "model2=joblib.load('models/Random_forest_2.pkl')\n",
    "\n",
    "\n",
    "for i in list_of_cluster:\n",
    "    cluster_data=data[data['cluster']==i]\n",
    "    \n",
    "    cluster_data=cluster_data.iloc[:,:-1]\n",
    "    \n",
    "    if i==0:\n",
    "        predict_0=model0.predict(cluster_data)\n",
    "        \n",
    "    elif i==1:\n",
    "        predict_1=model1.predict(cluster_data)\n",
    "    else:\n",
    "        predict_2=model2.predict(cluster_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98160339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33cb6777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b8b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
